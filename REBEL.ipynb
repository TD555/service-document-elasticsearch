{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaLEy7issHF-"
      },
      "source": [
        "# Install the relevant libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbNHzwH3kU4f",
        "outputId": "e807fd48-959f-48a8-f689-23a008f98178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in d:\\projects\\service for elasticsearch\\araks_venv\\lib\\site-packages (4.25.1)\n",
            "Requirement already satisfied: wikipedia in d:\\projects\\service for elasticsearch\\araks_venv\\lib\\site-packages (1.4.0)\n",
            "Requirement already satisfied: wikipedia-api in d:\\projects\\service for elasticsearch\\araks_venv\\lib\\site-packages (0.6.0)\n",
            "Requirement already satisfied: fuzzywuzzy[speedup] in d:\\projects\\service for elasticsearch\\araks_venv\\lib\\site-packages (0.18.0)\n",
            "Requirement already satisfied: filelock in d:\\projects\\service for elasticsearch\\araks_venv\\lib\\site-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in d:\\projects\\service for elasticsearch\\araks_venv\\lib\\site-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in d:\\projects\\service for elasticsearch\\araks_venv\\lib\\site-packages (from transformers) (1.26.1)\n",
            "Requirement already satisfied: packaging>=20.0 in d:\\projects\\service for elasticsearch\\araks_venv\\lib\\site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in d:\\projects\\service for elasticsearch\\araks_venv\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in d:\\projects\\service for elasticsearch\\araks_venv\\lib\\site-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in d:\\projects\\service for elasticsearch\\araks_venv\\lib\\site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in d:\\projects\\service for elasticsearch\\araks_venv\\lib\\site-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in d:\\projects\\service for elasticsearch\\araks_venv\\lib\\site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: beautifulsoup4 in d:\\projects\\service for elasticsearch\\araks_venv\\lib\\site-packages (from wikipedia) (4.12.2)\n",
            "Requirement already satisfied: python-levenshtein>=0.12 in d:\\projects\\service for elasticsearch\\araks_venv\\lib\\site-packages (from fuzzywuzzy[speedup]) (0.23.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in d:\\projects\\service for elasticsearch\\araks_venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (2023.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\projects\\service for elasticsearch\\araks_venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: Levenshtein==0.23.0 in d:\\projects\\service for elasticsearch\\araks_venv\\lib\\site-packages (from python-levenshtein>=0.12->fuzzywuzzy[speedup]) (0.23.0)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.1.0 in d:\\projects\\service for elasticsearch\\araks_venv\\lib\\site-packages (from Levenshtein==0.23.0->python-levenshtein>=0.12->fuzzywuzzy[speedup]) (3.5.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\projects\\service for elasticsearch\\araks_venv\\lib\\site-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\projects\\service for elasticsearch\\araks_venv\\lib\\site-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\projects\\service for elasticsearch\\araks_venv\\lib\\site-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\projects\\service for elasticsearch\\araks_venv\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: colorama in d:\\projects\\service for elasticsearch\\araks_venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in d:\\projects\\service for elasticsearch\\araks_venv\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.4.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers wikipedia fuzzywuzzy[speedup] wikipedia-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oMVVc3epkQbC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From d:\\PROJECTS\\Service for Elasticsearch\\araks_venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import math\n",
        "import torch\n",
        "import wikipediaapi\n",
        "import wikipedia\n",
        "import IPython\n",
        "from fuzzywuzzy import fuzz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkU8C2qVsLBh"
      },
      "source": [
        "# Load the REBEL model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DUOJhoVltPg",
        "outputId": "60eb8584-4af3-4127-ce3b-1ccf43f301ff"
      },
      "outputs": [],
      "source": [
        "# Load model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "nodes = ['Genetics']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "h-DboLAHkj_G"
      },
      "outputs": [],
      "source": [
        "async def similar(a, b):\n",
        "    return fuzz.token_set_ratio(a,b)\n",
        "\n",
        "async def extract_relations_from_model_output(text):\n",
        "    relations = []\n",
        "    relation, subject, relation, object_ = '', '', '', ''\n",
        "    text = text.strip()\n",
        "    current = 'x'\n",
        "    text_replaced = text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\")\n",
        "    for token in text_replaced.split():\n",
        "        if token == \"<triplet>\":\n",
        "            current = 't'\n",
        "            if relation != '':\n",
        "                relations.append({\n",
        "                    'head': subject.strip(),\n",
        "                    'type': relation.strip(),\n",
        "                    'tail': object_.strip()\n",
        "                })\n",
        "                relation = ''\n",
        "            subject = ''\n",
        "        elif token == \"<subj>\":\n",
        "            current = 's'\n",
        "            if relation != '':\n",
        "                relations.append({\n",
        "                    'head': subject.strip(),\n",
        "                    'type': relation.strip(),\n",
        "                    'tail': object_.strip()\n",
        "                })\n",
        "            object_ = ''\n",
        "        elif token == \"<obj>\":\n",
        "            current = 'o'\n",
        "            relation = ''\n",
        "        else:\n",
        "            if current == 't':\n",
        "                subject += ' ' + token\n",
        "            elif current == 's':\n",
        "                object_ += ' ' + token\n",
        "            elif current == 'o':\n",
        "                relation += ' ' + token\n",
        "    if subject != '' and relation != '' and object_ != '':\n",
        "            relations.append({\n",
        "                'head': subject.strip(),\n",
        "                'type': relation.strip(),\n",
        "                'tail': object_.strip()\n",
        "            })\n",
        "            \n",
        "    to_delete = []\n",
        "    for rel in relations:\n",
        "        for name in nodes:\n",
        "            if await similar(rel['head'].lower(), name.lower()) > 80 or similar(rel['tail'].lower(), name.lower()) > 80:\n",
        "                break\n",
        "        else:\n",
        "            to_delete.append(rel)\n",
        "\n",
        "    for rel_to_delete in to_delete:\n",
        "        relations.remove(rel_to_delete)\n",
        "\n",
        "          \n",
        "    return relations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqhLveaeuJNk"
      },
      "source": [
        "# Split spans: from long text to KB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "JqXxSWICJXBq"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import wikipediaapi\n",
        "import wikipedia\n",
        "\n",
        "class KB():\n",
        "    def __init__(self):\n",
        "        self.entities = {}\n",
        "        self.relations = []\n",
        "\n",
        "    def are_relations_equal(self, r1, r2):\n",
        "        return all(r1[attr] == r2[attr] for attr in [\"head\", \"type\", \"tail\"])\n",
        "\n",
        "    def exists_relation(self, r1):\n",
        "        return any(self.are_relations_equal(r1, r2) for r2 in self.relations)\n",
        "\n",
        "    async def merge_relations(self, r1):\n",
        "        r2 = [r for r in self.relations\n",
        "              if self.are_relations_equal(r1, r)][0]\n",
        "        spans_to_add = [span for span in r1[\"meta\"][\"spans\"]\n",
        "                        if span not in r2[\"meta\"][\"spans\"]]\n",
        "        r2[\"meta\"][\"spans\"] += spans_to_add\n",
        "    \n",
        "    async def check_wikipedia_entity(self, entity):\n",
        "        wiki_wiki = wikipediaapi.Wikipedia('Entity validation', 'en')  # Creating a Wikipedia object for English Wikipedia\n",
        "        page = wiki_wiki.page(entity)  # Get the Wikipedia page for the given entity\n",
        "        return page.exists()\n",
        "\n",
        "    async def get_wikipedia_data(self, candidate_entity):\n",
        "        try:\n",
        "            page = await asyncio.to_thread(wikipedia.page, candidate_entity, auto_suggest=False)\n",
        "            entity_data = {\n",
        "                \"title\": page.title,\n",
        "                \"url\": page.url,\n",
        "                \"summary\": page.summary\n",
        "            }\n",
        "            return entity_data\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def add_entity(self, e):\n",
        "        self.entities[e[\"title\"]] = {k:v for k,v in e.items() if k != \"title\"}\n",
        "\n",
        "    async def add_relation(self, r):\n",
        "        # check on wikipedia\n",
        "        candidate_entities = [r[\"head\"], r[\"tail\"]]\n",
        "        \n",
        "        check_entities = [ent if await self.check_wikipedia_entity(ent) else ent for ent in candidate_entities]\n",
        "        \n",
        "        if len(check_entities) != 2:\n",
        "            return\n",
        "        \n",
        "        entities = [await self.get_wikipedia_data(ent.replace(' ', '_')) if await self.get_wikipedia_data(ent)\n",
        "                    else {'title' : ent} for ent in check_entities]\n",
        "\n",
        "        # manage new entities\n",
        "        for e in entities:\n",
        "            self.add_entity(e)\n",
        "        \n",
        "        # rename relation entities with their wikipedia titles\n",
        "        r[\"head\"] = entities[0][\"title\"]\n",
        "        r[\"tail\"] = entities[1][\"title\"]\n",
        "\n",
        "        # manage new relation\n",
        "        if not self.exists_relation(r):\n",
        "            self.relations.append(r)\n",
        "        else:\n",
        "            await self.merge_relations(r)\n",
        "\n",
        "    def print(self):\n",
        "        print(\"Entities:\")\n",
        "        for e in self.entities.items():\n",
        "            print(f\"  {e}\")\n",
        "        print(\"Relations:\")\n",
        "        for r in self.relations:\n",
        "            print(f\"  {r}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "HlnLhlSmuUCs"
      },
      "outputs": [],
      "source": [
        "async def from_text_to_kb(text, span_length=128, verbose=False):\n",
        "    # tokenize whole text\n",
        "    inputs = tokenizer([text], return_tensors=\"pt\")\n",
        "\n",
        "    # compute span boundaries\n",
        "    num_tokens = len(inputs[\"input_ids\"][0])\n",
        "    if verbose:\n",
        "        print(f\"Input has {num_tokens} tokens\")\n",
        "    num_spans = math.ceil(num_tokens / span_length)\n",
        "    if verbose:\n",
        "        print(f\"Input has {num_spans} spans\")\n",
        "    overlap = math.ceil((num_spans * span_length - num_tokens) /\n",
        "                        max(num_spans - 1, 1))\n",
        "    spans_boundaries = []\n",
        "    start = 0\n",
        "    for i in range(num_spans):\n",
        "        spans_boundaries.append([start + span_length * i,\n",
        "                                 start + span_length * (i + 1)])\n",
        "        start -= overlap\n",
        "    if verbose:\n",
        "        print(f\"Span boundaries are {spans_boundaries}\")\n",
        "\n",
        "    # transform input with spans\n",
        "    tensor_ids = [inputs[\"input_ids\"][0][boundary[0]:boundary[1]]\n",
        "                  for boundary in spans_boundaries]\n",
        "    tensor_masks = [inputs[\"attention_mask\"][0][boundary[0]:boundary[1]]\n",
        "                    for boundary in spans_boundaries]\n",
        "    inputs = {\n",
        "        \"input_ids\": torch.stack(tensor_ids),\n",
        "        \"attention_mask\": torch.stack(tensor_masks)\n",
        "    }\n",
        "\n",
        "    # generate relations\n",
        "    num_return_sequences = 3\n",
        "    gen_kwargs = {\n",
        "        \"max_length\": 256,\n",
        "        \"length_penalty\": 0,\n",
        "        \"num_beams\": 3,\n",
        "        \"num_return_sequences\": num_return_sequences\n",
        "    }\n",
        "    generated_tokens = model.generate(\n",
        "        **inputs,\n",
        "        **gen_kwargs,\n",
        "    )\n",
        "\n",
        "    # decode relations\n",
        "    decoded_preds = tokenizer.batch_decode(generated_tokens,\n",
        "                                           skip_special_tokens=False)\n",
        "\n",
        "    # create kb\n",
        "    kb = KB()\n",
        "    i = 0\n",
        "    for sentence_pred in decoded_preds:\n",
        "        current_span_index = i // num_return_sequences\n",
        "        relations = await extract_relations_from_model_output(sentence_pred)\n",
        "        for relation in relations:\n",
        "            relation[\"meta\"] = {\n",
        "                \"spans\": [spans_boundaries[current_span_index]]\n",
        "            }\n",
        "            await kb.add_relation(relation)\n",
        "        i += 1\n",
        "\n",
        "    return kb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c13w1wTF61AB"
      },
      "source": [
        "# Filter and normalize entities with Wikipedia\n",
        "\n",
        "- remove all entities that doesn't have a page on Wikipedia\n",
        "- merge entities if they have the same wikipedia page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "nodes.append(\"Kafka\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEAiQNBhPU4U",
        "outputId": "660f2f3f-8cc0-4961-b223-be8baeb5712a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\PROJECTS\\Service for Elasticsearch\\araks_venv\\lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file d:\\PROJECTS\\Service for Elasticsearch\\araks_venv\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entities:\n",
            "  ('Genetics', {'url': 'https://en.wikipedia.org/wiki/Genetics', 'summary': 'Genetics is the study of genes, genetic variation, and heredity in organisms. It is an important branch in biology because heredity is vital to organisms\\' evolution. Gregor Mendel, a Moravian Augustinian friar working in the 19th century in Brno, was the first to study genetics scientifically. Mendel studied \"trait inheritance\", patterns in the way traits are handed down from parents to offspring over time. He observed that organisms (pea plants) inherit traits by way of discrete \"units of inheritance\". This term, still used today, is a somewhat ambiguous definition of what is referred to as a gene.\\nTrait inheritance and molecular inheritance mechanisms of genes are still primary principles of genetics in the 21st century, but modern genetics has expanded to study the function and behavior of genes. Gene structure and function, variation, and distribution are studied within the context of the cell, the organism (e.g. dominance), and within the context of a population. Genetics has given rise to a number of subfields, including molecular genetics, epigenetics and population genetics. Organisms studied within the broad field span the domains of life (archaea, bacteria, and eukarya).\\nGenetic processes work in combination with an organism\\'s environment and experiences to influence development and behavior, often referred to as nature versus nurture. The intracellular or extracellular environment of a living cell or organism may increase or decrease gene transcription. A classic example is two seeds of genetically identical corn, one placed in a temperate climate and one in an arid climate (lacking sufficient waterfall or rain). While the average height of the two corn stalks may be genetically determined to be equal, the one in the arid climate only grows to half the height of the one in the temperate climate due to lack of water and nutrients in its environment.'})\n",
            "  ('Molecular biology', {'url': 'https://en.wikipedia.org/wiki/Molecular_biology', 'summary': 'Molecular biology  is a branch of biology that seeks to understand the molecular basis of biological activity in and between cells, including biomolecular synthesis, modification, mechanisms, and interactions.Molecular biology was first described as an approach focused on the underpinnings of biological phenomenaâ€”uncovering the structures of biological molecules as well as their interactions, and how these interactions explain observations of classical biology.The term molecular biology was first used in 1945 by physicist William Astbury. In 1953 Francis Crick, James Watson, Rosalind Franklin, and colleagues working at the Medical Research Council Unit, Cavendish Laboratory, created the double helix model of DNA. They proposed the DNA structure based on previous research done by Franklin and Maurice Wilkins. This led to the discovery of DNA material in other microorganisms, plants, and animals.The field of molecular biology includes techniques which enable scientists to learn about molecular processes. These techniques are be used to efficiently target new drugs, diagnose disease, and better understand cell physiology. Some clinical research and medical therapies arising from molecular biology are covered under gene therapy whereas the use of molecular biology or molecular cell biology in medicine is now referred to as molecular medicine.\\n\\n'})\n",
            "Relations:\n",
            "  {'head': 'Genetics', 'type': 'part of', 'tail': 'Molecular biology', 'meta': {'spans': [[24, 152]]}}\n",
            "Entities:\n",
            "  ('Apache Kafka', {'url': 'https://en.wikipedia.org/wiki/Apache_Kafka', 'summary': 'Apache Kafka is a distributed event store and stream-processing platform. It is an open-source system developed by the Apache Software Foundation written in Java and Scala. The project aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. Kafka can connect to external systems (for data import/export) via Kafka Connect, and provides the Kafka Streams libraries for stream processing applications. Kafka uses a binary TCP-based protocol that is optimized for efficiency and relies on a \"message set\" abstraction that naturally groups messages together to reduce the overhead of the network roundtrip. This \"leads to larger network packets, larger sequential disk operations, contiguous memory blocks [...] which allows Kafka to turn a bursty stream of random message writes into linear writes.\"\\n\\n'})\n",
            "  ('distributed', {})\n",
            "  ('distributed event streaming', {})\n",
            "  ('publish-subscribe model', {})\n",
            "Relations:\n",
            "  {'head': 'Apache Kafka', 'type': 'use', 'tail': 'distributed', 'meta': {'spans': [[0, 128]]}}\n",
            "  {'head': 'Apache Kafka', 'type': 'use', 'tail': 'distributed event streaming', 'meta': {'spans': [[0, 128]]}}\n",
            "  {'head': 'Apache Kafka', 'type': 'use', 'tail': 'publish-subscribe model', 'meta': {'spans': [[0, 128]]}}\n"
          ]
        }
      ],
      "source": [
        "text1 = \"\"\"\n",
        "Biochemistry and molecular biology are used interchangeably in this book, and they are increasingly recognized as a unified discipline, rather than as two distinct disciplines or subdisciplines. Probably this is due to widespread adoption of the techniques of both disciplines by all biologists. However, historically molecular biology emerged from a convergence between biochemistry and genetics. Biochemistry is centred in chemistry and principally deals with understanding the function of proteins. Genetics is principally concerned with the function of genes and inheritance. Molecular biology emerged later, once the molecular mechanisms (biochemistry) of gene function were determined, and focuses on the relationship between genes and functional gene products (which are in most cases proteins). So molecular biology could be viewed as the nexus between biochemistry and genetics\n",
        "\"\"\"\n",
        "\n",
        "text2 = \"\"\"Apache Kafka is an open-source distributed event streaming platform used for building real-time data pipelines and streaming applications. It's designed to handle high volumes of data in a fault-tolerant and scalable manner. At its core, Kafka is built around the concept of a distributed commit log. It allows you to publish and subscribe to streams of records, store these records in a fault-tolerant way, and process them in real time or batch. Kafka operates on a publish-subscribe model, where producers publish data to topics, and consumers subscribe to those topics to process the data.\n",
        "\"\"\"\n",
        "\n",
        "# return_values = []\n",
        "\n",
        "# import threading\n",
        "# thread1 = threading.Thread(target=lambda: return_values.append(from_text_to_kb(text1)))\n",
        "# thread2 = threading.Thread(target=lambda: return_values.append(from_text_to_kb(text2)))\n",
        "\n",
        "# # Start both threads\n",
        "# thread1.start()\n",
        "# thread2.start()\n",
        "\n",
        "# # Join both threads to the main program\n",
        "# thread1.join()\n",
        "# thread2.join()\n",
        "\n",
        "# return_values\n",
        "# return_values[0].print()\n",
        "# return_values[1].print()\n",
        "\n",
        "import asyncio\n",
        "\n",
        "async def main():\n",
        "    task1 = asyncio.create_task(from_text_to_kb(text1))\n",
        "    task2= asyncio.create_task(from_text_to_kb(text2))\n",
        "\n",
        "    return await task1, await task2\n",
        "\n",
        "return_values = await main()\n",
        "\n",
        "return_values[0].print()\n",
        "return_values[1].print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
